---
title: "Statistical learning"
author: "Jinghan Zhao"
date: "2024-11-21"
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%",
  warning = FALSE
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(3)
```


## Try Lasso!!!

import and clean birthweight data

```{r}
bwt_df = 
  read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = case_match(babysex, 
                         1 ~ "male",
                         2 ~ "female"),
    babysex = fct_infreq(babysex),
    frace = case_match(frace,
                       1 ~ "white",
                       2 ~ "black",
                       3 ~ "asian",
                       4 ~ "puerto rican",
                       8 ~ "other"),
    frace = fct_infreq(frace),
    mrace = case_match(mrace,
                       1 ~ "white",
                       2 ~ "black",
                       3 ~ "asian",
                       4 ~ "puerto rican",
                       8 ~ "other"),
    mrace = fct_infreq(mrace)
  ) %>% 
  sample_n(200)
```


Construct inputs for `glmnet`

`model.matrix` expanding factors to a set of dummy variables.

```{r}
x = model.matrix(bwt ~ ., data = bwt_df)[, -1]
y = bwt_df %>% pull(bwt)
```


Fit lasso for several lambdas

`lasso_fit` is one certain fit

`lasso_cv` does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda

`lasso_opt` is the min lambda chosen from cross-validation

```{r}
lambda = 10^seq(-2, 2.75, by = 0.1)

lasso_fit = 
  glmnet(x = x, y = y, lambda = lambda)

lasso_cv = 
  cv.glmnet(x = x, y = y, lambda = lambda)

lambda_opt = lasso_cv[["lambda.min"]]
```

Usual lasso plot

```{r}
lasso_fit %>% 
  broom::tidy() %>% 
  filter(term != "(Intercept)") %>% 
  select(term, lambda, estimate) %>% 
  complete(term, lambda, fill = list(estimate = 0)) %>% 
  ggplot(aes(x = lambda, y = estimate, group = term, color = term)) +
  geom_vline(xintercept = lambda_opt, color = "red") +
  geom_line()
```


```{r}
final_lasso_fit = 
  glmnet(x = x, y = y, lambda = lambda_opt)

final_lasso_fit %>% 
  broom::tidy()
```



## Cluster pokemons


```{r}
pokemon_df = 
  read_csv("data/pokemon.csv") %>% 
  janitor::clean_names() %>% 
  select(hp, speed)
```

```{r}
pokemon_df %>% 
  ggplot(aes(x = hp, y = speed)) +
  geom_point()
```


Let's use kmeans to cluster these pokemon!!

```{r}
kmeans_fit = 
  kmeans(x = pokemon_df, centers = 4)
```

Can I plot these results?

If have 3+ variables, it'll be hard to plot and tell if clustering works well (clustering needs plot!)

```{r}
pokemon_df =  
  broom::augment(kmeans_fit, pokemon_df)

pokemon_df %>% 
  ggplot(aes(x = hp, y = speed, color = .cluster)) +
  geom_point()
```

